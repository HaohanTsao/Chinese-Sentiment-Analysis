{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Bert Embedding for Sensitivity Classification\n"],"metadata":{"id":"rUV5B8UdUKuM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"90QQZWW1UF6q"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import jieba\n","import re\n","import torch"]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3dip557dlLR","executionInfo":{"status":"ok","timestamp":1684205378946,"user_tz":-480,"elapsed":16211,"user":{"displayName":"曹灝翰","userId":"00262753423893401071"}},"outputId":"d27115a8-947e-43bc-c1db-dbb895e60b3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/python機器學習專案')"],"metadata":{"id":"cYmnpARPoU5I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["載入chinese-bert-wwm 模型\n","src = https://github.com/ymcui/Chinese-BERT-wwm#%E4%B8%AD%E6%96%87%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD"],"metadata":{"id":"_jpxrwJUfSwp"}},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","\n","model_name = 'hfl/chinese-bert-wwm'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","bert_pretrained_model = BertModel.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rkxt3n02dOev","executionInfo":{"status":"ok","timestamp":1684205766283,"user_tz":-480,"elapsed":2445,"user":{"displayName":"曹灝翰","userId":"00262753423893401071"}},"outputId":"3a4fa4ac-003e-4897-c090-80e33cf0220f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# 試用一下\n","text = \"今天天氣真好\"\n","\n","# tokenizer 會把文字變成數字\n","tokenized_input = tokenizer.encode(text, add_special_tokens=True)\n","print(\"Encoded input:\", tokenized_input)\n","\n","# bert_pretrained_model要求輸入是PyTorch tensor\n","input_ids = torch.tensor([tokenized_input])\n","\n","# 丟給model 會return一個很多東西的結果\n","outputs = bert_pretrained_model(input_ids)\n","print('====================================')\n","print('outputs裡的東西:')\n","print(dir(outputs))\n","\n","# 文字向量在last_hidden_state\n","embeddings = outputs.last_hidden_state\n","\n","# 偷看一下向量會長怎樣\n","print('===================================')\n","print(\"Embeddings shape:\", embeddings.size())\n","print('')\n","# 前5個\n","print(\"Embeddings values:\")\n","for i in range(8):\n","  print(f\"第{i}個字的前五個向量:\")\n","  print(embeddings[0][i][:5].tolist())\n","  print('')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12Rj5utPfKvy","executionInfo":{"status":"ok","timestamp":1684208026266,"user_tz":-480,"elapsed":927,"user":{"displayName":"曹灝翰","userId":"00262753423893401071"}},"outputId":"85003142-356d-4efb-d3a9-86db9aa81b23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded input: [101, 791, 1921, 1921, 3706, 4696, 1962, 102]\n","====================================\n","outputs裡的東西:\n","['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'attentions', 'clear', 'copy', 'cross_attentions', 'fromkeys', 'get', 'hidden_states', 'items', 'keys', 'last_hidden_state', 'move_to_end', 'past_key_values', 'pooler_output', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n","===================================\n","Embeddings shape: torch.Size([1, 8, 768])\n","\n","Embeddings values:\n","第0個字的前五個向量:\n","[0.7353697419166565, 0.29999643564224243, 0.1433165818452835, 0.35470569133758545, 0.7991356253623962]\n","\n","第1個字的前五個向量:\n","[0.6588079929351807, 0.34962716698646545, 0.11939653754234314, 0.27863821387290955, 1.2953853607177734]\n","\n","第2個字的前五個向量:\n","[0.6876529455184937, -0.0677679255604744, 0.001761157065629959, 0.052672576159238815, 1.0232077836990356]\n","\n","第3個字的前五個向量:\n","[1.2725307941436768, -0.07347938418388367, -0.16768787801265717, 0.20785477757453918, 0.20453085005283356]\n","\n","第4個字的前五個向量:\n","[0.6127408146858215, -0.40566420555114746, -0.5822169184684753, 0.43012702465057373, -0.010332583449780941]\n","\n","第5個字的前五個向量:\n","[0.8257439136505127, -0.3273986279964447, 0.041862912476062775, 0.5747068524360657, 0.4482441246509552]\n","\n","第6個字的前五個向量:\n","[1.0089728832244873, -0.09843745082616806, -0.3202238976955414, 0.44892260432243347, 0.6592344045639038]\n","\n","第7個字的前五個向量:\n","[0.7891004085540771, 0.3886559009552002, 0.09364668279886246, 0.9627493619918823, 0.5339004993438721]\n","\n"]}]},{"cell_type":"code","source":["# 儲存模型\n","bert_pretrained_model.save_pretrained(\"/content/drive/MyDrive/python機器學習專案/model_file\")\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/python機器學習專案/model_file\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-VUNqnYnzNa","executionInfo":{"status":"ok","timestamp":1684208284157,"user_tz":-480,"elapsed":7075,"user":{"displayName":"曹灝翰","userId":"00262753423893401071"}},"outputId":"f9c2b83c-d378-402f-8df7-005ac9d745f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/python機器學習專案/model_file/tokenizer_config.json',\n"," '/content/drive/MyDrive/python機器學習專案/model_file/special_tokens_map.json',\n"," '/content/drive/MyDrive/python機器學習專案/model_file/vocab.txt',\n"," '/content/drive/MyDrive/python機器學習專案/model_file/added_tokens.json')"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["#　嘗試從資料夾中下載\n","model_temp = BertModel.from_pretrained(\"/content/drive/MyDrive/python機器學習專案/model_file\")\n","tokenizer_temp = BertTokenizer.from_pretrained(\"/content/drive/MyDrive/python機器學習專案/model_file\")"],"metadata":{"id":"Wyl_WUtro8qJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 試用\n","text = \"今天天氣真好\"\n","\n","# tokenizer 會把文字變成數字\n","tokenized_input = tokenizer_temp.encode(text, add_special_tokens=True)\n","print(\"Encoded input:\", tokenized_input)\n","\n","# bert_pretrained_model要求輸入是PyTorch tensor\n","input_ids = torch.tensor([tokenized_input])\n","\n","# 丟給model 會return一個很多東西的結果\n","outputs = model_temp(input_ids)\n","print('====================================')\n","print('outputs裡的東西:')\n","print(dir(outputs))\n","\n","# 文字向量在last_hidden_state\n","embeddings = outputs.last_hidden_state\n","\n","# 偷看一下向量會長怎樣\n","print('===================================')\n","print(\"Embeddings shape:\", embeddings.size())\n","print('')\n","# 前5個\n","print(\"Embeddings values:\")\n","for i in range(8):\n","  print(f\"第{i}個字的前五個向量:\")\n","  print(embeddings[0][i][:5].tolist())\n","  print('')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oub1Dlg8pZ9H","executionInfo":{"status":"ok","timestamp":1684208518875,"user_tz":-480,"elapsed":414,"user":{"displayName":"曹灝翰","userId":"00262753423893401071"}},"outputId":"9b3986d4-6c6c-4700-87bf-85223e67ade6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded input: [101, 791, 1921, 1921, 3706, 4696, 1962, 102]\n","====================================\n","outputs裡的東西:\n","['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'attentions', 'clear', 'copy', 'cross_attentions', 'fromkeys', 'get', 'hidden_states', 'items', 'keys', 'last_hidden_state', 'move_to_end', 'past_key_values', 'pooler_output', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n","===================================\n","Embeddings shape: torch.Size([1, 8, 768])\n","\n","Embeddings values:\n","第0個字的前五個向量:\n","[0.7353697419166565, 0.29999643564224243, 0.1433165818452835, 0.35470569133758545, 0.7991356253623962]\n","\n","第1個字的前五個向量:\n","[0.6588079929351807, 0.34962716698646545, 0.11939653754234314, 0.27863821387290955, 1.2953853607177734]\n","\n","第2個字的前五個向量:\n","[0.6876529455184937, -0.0677679255604744, 0.001761157065629959, 0.052672576159238815, 1.0232077836990356]\n","\n","第3個字的前五個向量:\n","[1.2725307941436768, -0.07347938418388367, -0.16768787801265717, 0.20785477757453918, 0.20453085005283356]\n","\n","第4個字的前五個向量:\n","[0.6127408146858215, -0.40566420555114746, -0.5822169184684753, 0.43012702465057373, -0.010332583449780941]\n","\n","第5個字的前五個向量:\n","[0.8257439136505127, -0.3273986279964447, 0.041862912476062775, 0.5747068524360657, 0.4482441246509552]\n","\n","第6個字的前五個向量:\n","[1.0089728832244873, -0.09843745082616806, -0.3202238976955414, 0.44892260432243347, 0.6592344045639038]\n","\n","第7個字的前五個向量:\n","[0.7891004085540771, 0.3886559009552002, 0.09364668279886246, 0.9627493619918823, 0.5339004993438721]\n","\n"]}]},{"cell_type":"markdown","source":["# GOOD!\n"],"metadata":{"id":"8eLLujx6pqzR"}},{"cell_type":"markdown","source":["根據https://github.com/ymcui/Chinese-BERT-wwm#%E4%B8%AD%E6%96%87%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD\n","\n","繁體中文是可以直接使用BERT-wwm, 表現不會比簡體差"],"metadata":{"id":"qbiPwkg0p5kb"}}]}